# ğŸ¬ Tamil Film LoRA Fineâ€‘Tuning (gptâ€‘ossâ€‘20b)

This project demonstrates how to fineâ€‘tune the **gptâ€‘ossâ€‘20b** model with **LoRA (Lowâ€‘Rank Adaptation)** on a Tamil film dataset. The fineâ€‘tuned model generates engaging and spoilerâ€‘free film plots based on **actor** and **genres**.

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_tamil_movies.jsonl   # raw dataset (10 Tamil films)
â”‚   â”œâ”€â”€ sft_train.jsonl             # processed instruction dataset
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_dataset.py          # converts raw data â†’ SFT format
â”‚   â”œâ”€â”€ train_lora.py               # fineâ€‘tuning script
â”‚   â”œâ”€â”€ infer.py                    # inference script
â””â”€â”€ checkpoints/
    â””â”€â”€ gpt_oss_20b_tamil_lora/     # trained LoRA adapters
```

---

## âš™ï¸ Installation

```bash
pip install -r requirements.txt
```

> ğŸ’¡ A GPU with 24GB+ VRAM is recommended. For smaller GPUs, adjust batch size, LoRA rank, or sequence length.

---

## ğŸš€ Workflow

### 1. Prepare Dataset

Convert raw dataset to instructionâ€‘tuning format:

```bash
python scripts/prepare_dataset.py
```

### 2. Train LoRA Adapter

Login to Huggingface using your token.

```bash
hf auth login
```


Fineâ€‘tune the model with LoRA:

```bash
python scripts/train_lora.py \
  --model_name openai/gpt-oss-20b \
  --data_path data/sft_train.jsonl \
  --output_dir checkpoints/gpt_oss_20b_tamil_lora \
  --epochs 3 --per_device_batch_size 1 --gradient_accumulation_steps 8
```

### 3. Run Inference

Generate a plot using the trained adapter:

```bash
python scripts/infer.py
```

Example output:

```
=== Generated Plot ===
A principled bus conductor is pushed into vigilantism when a land mafia threatens his township...
```

---

## ğŸ“˜ Dataset Format

Each entry in the raw dataset includes an actor, genres, and plot:

```json
{
  "actor": "Vijay",
  "genres": ["action", "drama"],
  "plot": "A principled bus conductor is pushed into vigilantism..."
}
```

After running `prepare_dataset.py`, entries are transformed into **instructionâ€‘style JSONL** for SFT training.

---

## ğŸ› ï¸ Notes

* Adjust LoRA parameters (`--lora_r`, `--lora_alpha`) based on VRAM.
* Use `bfloat16` or `float16` precision for efficiency.
* Extend dataset for stronger generalization.

---

## ğŸ™Œ Acknowledgements

* **Hugging Face Transformers** for model loading & training utilities.
* **PEFT (Parameterâ€‘Efficient Fineâ€‘Tuning)** for LoRA.
* Inspired by Tamil cinema storytelling traditions.
